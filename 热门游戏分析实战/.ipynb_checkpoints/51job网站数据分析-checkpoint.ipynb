{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.数据爬取\n",
    "#### 背景   \n",
    "对北京市数据分析行业的信息了解,更好实现择业\n",
    "#### 任务说明\n",
    "使用Scrapy实现对51job网站数据分析工程师相关职位爬取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 scrapy介绍\n",
    "Scrapy是用纯Python实现一个为了爬取网站数据、提取结构性数据而编写的应用框架，用户只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。\n",
    "\n",
    "#### 1.2 scarpy架构图\n",
    "![\"架构图\"](imgs/架构图.webp)\n",
    "\n",
    "#### 1.3 架构说明\n",
    "* crapy Engine(引擎): 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等\n",
    "* Scheduler(调度器): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎\n",
    "* Downloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理\n",
    "* Spider（爬虫）：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)\n",
    "* Item Pipeline(管道)：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方\n",
    "* Downloader Middlewares（下载中间件）：自定义扩展下载功能的组件\n",
    "* Spider Middlewares（Spider中间件）：可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests）\n",
    "\n",
    "#### 1.4  爬虫项目创建\n",
    "    * 创建爬虫项目，命令：scrapy startproject 项目名称\n",
    "    * 创建爬虫文件，命令：scrapy genspider 文件名称 域名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item.py\n",
    "import scrapy\n",
    "\n",
    "class Demo01Item(scrapy.Item):\n",
    "    job_name = scrapy.Field()\n",
    "    job_company_name = scrapy.Field()\n",
    "    job_place = scrapy.Field()\n",
    "    job_salary = scrapy.Field()\n",
    "    job_time = scrapy.Field()\n",
    "    detail_url = scrapy.Field()\n",
    "    job_msg = scrapy.Field()\n",
    "    job_comm = scrapy.Field()\n",
    "    job_con = scrapy.Field()\n",
    "    com_flag = scrapy.Field()\n",
    "    com_peo = scrapy.Field()\n",
    "    com_trade = scrapy.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# piplines.py\n",
    "class Demo01Pipeline(object):\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        job_name = item['job_name']\n",
    "        job_company_name = item['job_company_name']\n",
    "        job_place = item['job_place']\n",
    "        job_salary = item['job_salary']\n",
    "        job_time = item['job_time']\n",
    "        job_msg = item['job_msg']\n",
    "        job_comm = item['job_comm']\n",
    "        job_con = item['job_con']\n",
    "        com_flag = item['com_flag']\n",
    "        com_peo = item['com_peo']\n",
    "        com_trade = item['com_trade']\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取数据核心代码\n",
    "import scrapy\n",
    "from demo01.items import Demo01Item\n",
    "\n",
    "class A51jobSpider(scrapy.Spider):\n",
    "    name = '51job'\n",
    "    allowed_domains = ['51job.com']\n",
    "    start_urls = ['https://search.51job.com/list/010000,000000,0000,00,9,99,%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590,2,6.html?lang=c&postchannel=0000&workyear=99&cotype=99&degreefrom=99&jobterm=99&companysize=99&ord_field=0&dibiaoid=0&line=&welfare=']\n",
    "\n",
    "    def parse(self, response):\n",
    "        yield scrapy.Request(\n",
    "            url=response.url,\n",
    "            callback=self.parse_job_info,\n",
    "            meta={},\n",
    "            dont_filter=True\n",
    "        )\n",
    "\n",
    "    # 下一页的网址信息\n",
    "    def parse_next_page(self, response):\n",
    "        next_page = response.xpath(\"//li[@class='bk'][2]/a/@href\").extract_first('')\n",
    "        if next_page:\n",
    "            yield scrapy.Request(\n",
    "                url=next_page,\n",
    "                callback=self.parse_job_info,\n",
    "                meta={},\n",
    "                dont_filter=True\n",
    "            )\n",
    "\n",
    "    # 详情爬取\n",
    "    def detail_parse(self,response):\n",
    "        # 接收上级已爬取的数据\n",
    "        item = response.meta['item']\n",
    "        job_msg = response.xpath(\"//div[@class='cn']/p[@class='msg ltype']/@title\").extract()\n",
    "        job_comm = response.xpath(\"//div[@class='jtag']//span/text()\").extract()\n",
    "        job_con = response.xpath(\"//div[@class='bmsg inbox']/p/text()\").extract()\n",
    "        com_flag = response.xpath(\"//div[@class='com_tag']/p[1]/text()\").extract()\n",
    "        com_peo = response.xpath(\"//div[@class='com_tag']/p[2]/text()\").extract()\n",
    "        com_trade = response.xpath(\"//div[@class='com_tag']/p[3]/@title\").extract()\n",
    "        item['job_msg'] = job_msg\n",
    "        item['job_comm'] = job_comm\n",
    "        item['job_con'] = job_con\n",
    "        item['com_flag'] = com_flag\n",
    "        item['com_peo'] = com_peo\n",
    "        item['com_trade'] = com_trade\n",
    "        return item\n",
    "\n",
    "\n",
    "    # 解析并封装数据到item中\n",
    "    def parse_job_info(self, response):\n",
    "        # 读取所有的招聘信息列表\n",
    "        job_div_list = response.xpath(\"//div[@id='resultList']/div[@class='el']\")\n",
    "        for job_div in job_div_list:\n",
    "            # 遍历获取每项\n",
    "            job_name = job_div.xpath(\"p/span/a/@title\").extract_first('无工作名称').strip().replace(\",\", \"/\")\n",
    "            job_company_name = job_div.xpath(\"span[@class='t2']/a/@title\").extract_first('无公司名称').strip()\n",
    "            job_place = job_div.xpath(\"span[@class='t3']/text()\").extract_first('无地点名称').strip()\n",
    "            job_salary = job_div.xpath(\"span[@class='t4']/text()\").extract_first('面议').strip()\n",
    "            job_time = job_div.xpath(\"span[@class='t5']/text()\").extract_first('无时间信息').strip()\n",
    "            detail_url = job_div.xpath(\"p/span/a/@href\").extract_first().strip()\n",
    "\n",
    "            # 数据封装到item中\n",
    "            item = Demo01Item()\n",
    "            item['job_name'] = job_name\n",
    "            item['job_company_name'] = job_company_name\n",
    "            item['job_place'] = job_place\n",
    "            item['job_salary'] = job_salary\n",
    "            item['job_time'] = job_time\n",
    "            item['detail_url'] = detail_url\n",
    "            # 爬取内页数据\n",
    "            yield scrapy.Request(item['detail_url'], meta={'item': item}, callback=self.detail_parse)\n",
    "            yield item\n",
    "\n",
    "        # 发送请求获取下一页\n",
    "        yield scrapy.Request(\n",
    "            url=response.url,\n",
    "            callback=self.parse_next_page,\n",
    "            dont_filter=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行爬取\n",
    "execute(\"scrapy crawl 51job\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.数据清洗\n",
    "#### 字段说明\n",
    "*  com_flag: 企业类型\n",
    "*  com_peo: 企业规模\n",
    "*  com_trade: 企业类型\n",
    "*  job_comm: 附加信息\n",
    "*  job_company: 公司名称\n",
    "*  job_con: 公司位置\n",
    "*  job_msg: 工作信息(招聘人数,经验,学历等)\n",
    "*  job_name: 职位名称\n",
    "*  job_salary: 工资\n",
    "*  job_time: 发布时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 导入相关的库\n",
    "导入需要的库，同时，进行一些初始化的设置 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 支持中文\n",
    "mpl.rcParams[\"font.family\"] = \"SimHei\"\n",
    "mpl.rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 加载相关的数据集\n",
    "加载相关的数据集（注意原数据集中是否存在标题），并查看数据的大致情况。\n",
    "可以使用head / tail，也可以使用sample。\n",
    "列没有显式完整，我们需要进行设置。（pd.set_option）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"job.csv\",encoding=\"gbk\")\n",
    "pd.set_option(\"max_columns\",100)\n",
    "# 去掉detail_url\n",
    "# df = df.drop(\"detail_url\",axis=1)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 数据探索和清洗\n",
    "##### 2.3.1 缺失值处理\n",
    "* 通过info查看缺失值信息（以及每列的类型信息）。\n",
    "* 可以通过isnull, any, dropna，fillna等方法结合使用，对缺失值进行处理。\n",
    "\n",
    "##### 2.3.2 异常值处理\n",
    "* 通过describe查看数值信息。\n",
    "* 可配合箱线图辅助。\n",
    "\n",
    "##### 2.3.3 重复值处理\n",
    "* 使用duplicate检查重复值。可配合keep参数进行调整。\n",
    "* 使用drop_duplicate删除重复值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现去重\n",
    "df=df[df[\"job_company_name\"].duplicated()]\n",
    "\n",
    "# 去除无效的数据(与数据分析差距较大的内容)\n",
    "df=df[df[\"job_name\"].str.contains(r'.*?数据.*?|.*?分析.*?')]\n",
    "\n",
    "# 将job_msg字段进行切分并存储到新的csv文件中\n",
    "jobMsg = df[\"job_msg\"].str.split(\",\",expand=True)\n",
    "jobMsg.columns = [\"job_place\",\"work_expr\",\"edu\",\"hire_count\",\"pub_time\",\"additional1\",\"additional2\"]\n",
    "df = df.join(jobMsg)\n",
    "# 存储处理之后的内容到文件中\n",
    "df.to_csv('newjob.csv',encoding=\"gbk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将薪资列进行处理,处理为最低和最高信息两列,并将其转换为float类型\n",
    "# 1.1 对薪资列进行处理,统一处理单位为千/月,并且分为最高工资和最低工资\n",
    "df = pd.read_csv(\"newjob.csv\",encoding=\"gbk\")\n",
    "pd.set_option(\"max_columns\",100)\n",
    "\n",
    "data = df.copy()\n",
    "t = data[data[\"job_salary\"] != \"面议\"]\n",
    "low = []\n",
    "high = []\n",
    "# 对薪资进行处理\n",
    "for salary in t[\"job_salary\"]:\n",
    "    low_salary=re.findall(re.compile('(\\d*\\.?\\d+)'),salary)[0]\n",
    "    if '-'in salary: #针对1-2万/月或者10-20万/年的情况，包含-\n",
    "        high_salary=re.findall(re.compile('(\\d?\\.?\\d+)'),salary)[1]\n",
    "        if u'万' in salary and u'年' in salary:#单位统一成千/月的形式\n",
    "            low_salary = round(float(low_salary) / 12 * 10,2)\n",
    "            high_salary = round(float(high_salary) / 12 * 10,2)\n",
    "        elif u'万' in salary and u'月' in salary:\n",
    "            low_salary = round(float(low_salary) * 10,2)\n",
    "            high_salary = round(float(high_salary) * 10,2)\n",
    "        elif u'千'in salary and u'月'in salary:\n",
    "            low_salary = round(float(low_salary),2)\n",
    "            high_salary = round(float(high_salary),2)\n",
    "        high.append(high_salary)\n",
    "        \n",
    "    else: #针对20万以上/年和100元/天这种情况，不包含-，取最低工资，没有最高工资\n",
    "        if u'万' in salary and u'年' in salary:#单位统一成千/月的形式\n",
    "            low_salary = round(float(low_salary) / 12 * 10,2)\n",
    "        elif u'万' in salary and u'月' in salary:\n",
    "            low_salary = round(float(low_salary) * 10,2)\n",
    "        elif u'元'in salary and u'天'in salary:\n",
    "            low_salary=round(float(low_salary)/1000*21,2)   #每月工作日21天\n",
    "        elif u'千'in salary and u'月'in salary:\n",
    "            low_salary=round(float(low_salary),2)     \n",
    "        high_salary = low_salary\n",
    "        high.append(high_salary)\n",
    "    low.append(low_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.数据分析案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 求最低工资分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求最低值的直方图\n",
    "low.sort()\n",
    "\n",
    "# plt.boxplot(low)\n",
    "# 删除偏离值\n",
    "del low[len(low)-1]\n",
    "\n",
    "plt.xlim(low[0],low[len(low)-1])\n",
    "plt.hist(low,bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 求最高工资分布状况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high.sort()\n",
    "\n",
    "# plt.boxplot(high)\n",
    "# 删除偏离值\n",
    "del high[len(high)-1]\n",
    "\n",
    "plt.xlim(high[0],high[len(high)-1])\n",
    "plt.hist(high,bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 数据分析行业对学历的要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.copy()\n",
    "eduCount = t.groupby(\"edu\")[\"edu\"].count()\n",
    "eduCount.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 数据分析职位所在企业类型的分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.copy()\n",
    "comFlag =  t.groupby(\"com_flag\")[\"com_flag\"].count()\n",
    "comFlag.plot(kind=\"pie\",autopct=\"%.2f%%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 行业的企业规模状况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.copy()\n",
    "com_peo =  t.groupby(\"com_peo\")[\"com_peo\"].count()\n",
    "com_peo.plot(kind=\"pie\",autopct=\"%.2f%%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 数据分析对工作经验的要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.copy()\n",
    "hire_count =  t.groupby(\"hire_count\")[\"hire_count\"].count()\n",
    "hire_count.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 数据分析需求行业的比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.copy()\n",
    "com_trade =  t.groupby(\"com_trade\")[\"com_trade\"].count()\n",
    "# com_trade.plot(kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思考\n",
    "工资与学历已经工作经验的相关性问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩展\n",
    "实现对全国的数据分析行业数据分析,查看地区对行业的需求以及薪资的影响"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
